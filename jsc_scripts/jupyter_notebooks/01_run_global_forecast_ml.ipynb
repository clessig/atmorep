{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772a178b-63df-4de4-b293-7a5ed4cdac0f",
   "metadata": {},
   "source": [
    "# Tutorial 1 - Forecasting with AtmoRep\n",
    "\n",
    "## Preamble\n",
    "\n",
    "Training of AtmoRep is a computationally intensive process, typically requiring a couple of weeks on multiple compute nodes of Juwels Booster. \n",
    "However, a selection of pretrained AtmoRep models (singleformers and multiformers) has been made available, enabling immediate use in so called zero-shot applications. <br>\n",
    "Zero-shot applications leverage the intrinsic capabilities of the AtmoRep model and do not require task-specific finetuning or the addition of a specialized network to the core model.\n",
    "In particular, AtmoRep has demonstrated skill for short-range forecasting/nowcasting, temporal interpolation and data decompression. <br>\n",
    "In the following, we will explore the forecast application in a zero-shot setting.\n",
    "\n",
    "#### Objectives\n",
    "The objectives of this tutorial comprise\n",
    "- Generation of a six-hour forecast with AtmoRep in inference mode\n",
    "- Handling and plotting AtmoRep's model output\n",
    "- Evaluation AtmoRep's forecast capacities\n",
    "\n",
    "### Prerequisites\n",
    "To run the following cells of the Jupyter Notebook, please ensure that you fulfill the following requirements:\n",
    "- Run your JupyterLab on the GPU node of JURECA (use the `dc-gpu`-partition)\n",
    "- Activate your hackathon kernel (see upper right corner of your Jupyter Notebook)\n",
    "\n",
    "## Step 1 - Generate a forecast with AtmoRep\n",
    "\n",
    "### Forecast Inference in AtmoRep\n",
    "\n",
    "In this section, we will demonstrate how to perform a **global forecast** using AtmoRep. The global data is separated into tiles/neighborhoods using a spatiotemporal grids, the illustration bellow shows how a single tile is processed during a forecast. The data is represented across latitude, longitude, and time dimensions. Forecasting involves predicting future tokens (y) within a subset of this grid based on past observations (x). This procedure allows us to evaluate the model's ability to extrapolate patterns and generate meaningful predictions for unseen temporal data.\n",
    "\n",
    "![forecast](../img/forecast_1ml.jpg \"Data processing during forecast, with x input, y target output\")\n",
    "\n",
    "### To-Do\n",
    "- [x] get pretrained models \n",
    "- [x] ensure that data is linked\n",
    "- [x] parse user-config to enable user-specific output and model paths\n",
    "\n",
    "We start by importing some Python packages and classes from the AtmoRep code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a502c36-8bd1-44f2-ade9-b46c3805b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from atmorep.core.evaluator import Evaluator\n",
    "from atmorep.config.config import UserConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d8468-0950-4bf0-963a-d3ca41159559",
   "metadata": {},
   "source": [
    "Before continuing, we need to set-up a user-specific configuration. <br>\n",
    "This procedures provides both, a symbolic link to the directories where pretrained models and the ERA5-data (in zarr stores) are available and the configuration of user-specific output directories for their (custom) models, their inference results (e.g. global forecasts) and their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38039ab2-48af-43d3-b6cf-7ce3babea619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! ADAPT PATH TO YOUR OWN DIRECTORY!!!\n",
    "project_dir = Path(\"/p/project/deepacf/atmo-rep/langguth1/atmo-rep/\")                    \n",
    "user_config = UserConfig.from_path(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b27bd4-3e31-4f4f-ada6-eb8f4b12ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_config.__dict__[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd976890-6c74-4c31-8ddd-885899a807eb",
   "metadata": {},
   "source": [
    "In the next step, we are already ready to configure an instance of the `Evaluator`-class. \n",
    "This class allows users to perform inference on pretrained (either provided or custom) models. The class loads a pretrained model which is identified via its W&B-ID and then performs the inference step, i.e. only the forward step of the model is executed. The inference output is saved to the `results`-directory under the user-specific project directory. <br> \n",
    "Note that the `Evaluator`-class supports various modes for the inference (e.g. global forecasting) as outlined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf449818-d5fc-489d-9ee1-6d4786ee37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='wc5e2i3t' # pre-trained multiforner\n",
    "\n",
    "mode, options = 'global_forecast', {\n",
    "                                  'dates' : [[2021, 2, 10, 12], [2021, 2, 11, 0]],    # this corresponds to teh last time step of the sequence \n",
    "                                  'token_overlap' : [0, 0],\n",
    "                                  'forecast_num_tokens' : 2,        # corresponds to a 6h-forecast (=2x3h)\n",
    "                                  'with_pytest' : False }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502ad5d-a6c3-4359-8be8-f026d520bb18",
   "metadata": {},
   "source": [
    "For each inference experiment, three key parameters are required:\n",
    "- The `model_id`, which identifies the pre-trained model used for inference\n",
    "- The `mode` of inference you wish to perform\n",
    "- The required attributes, labeled as `options`, necessary to carry out the selected inference mode\n",
    "\n",
    "In the case of `global_forecast`, Atmorep requires the following options:\n",
    "- `'dates'`: This is a list of dates. For a single date, it should still be provided as a list containing just that one item. Each date should follow the format `[year, month, day, hour]`, representing the last timestep to be forecasted. Atmorep will load data from 36 hours before this timestamp (inclusive), then start forecasting `forecast_num_tokens` $\\times$ 3 hours backwards.\n",
    "- `'token_overlap'`: Specifies the degree of overlap between tiles (or neighborhoods).\n",
    "- `'forecast_num_tokens'`: The number of tokens to be forecasted.\n",
    "- `'with_pytest'`: Enables or disables systematic testing processes. \n",
    "\n",
    "After choosing the appropriate mode for our application (global forecasting) and setting the configuration options, we can run the inference by calling the `evaluate`-method of the class object. \n",
    "Note that the following process takes a couple of minutes to complete the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907e92e-4f36-4fb2-adbe-e6720b20942c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"MASTER_ADDR\"] = str(os.environ['SLURM_JOB_NODELIST'])       # AtmoRep needs to know the hostname of the master-node\n",
    "\n",
    "# run inference\n",
    "now = time.time()\n",
    "Evaluator.evaluate( mode, model_id, options, user_config=user_config)\n",
    "print(\"time\", time.time() - now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f4c14-9ae3-48dd-b0d5-ab12922995b4",
   "metadata": {},
   "source": [
    "Ensure to track the W&B-ID that is outputted when running the inference. This ID is required to run the subsequent validation (Step 2 of this tutorial).\n",
    "\n",
    "### To-Dos @Asma:\n",
    "- [x] Add details about the `global_forecast`-mode, i.e. the required tiling to get a global field with the local neighborhood approach by AtmoRep, the relevance of the `forecast_num_tokens`-parameter etc.\n",
    "- [x] Provide insight into the different modes of the Evaluator, i.e. what happens with the `forecast`-mode (difference to global_forecast), what is done for temproal interpolation\n",
    "- [x] Verify what `'num_samples_validate': 128` corresponds to number tiles/neighborhoods\n",
    "- [x] It would probably also nice to have a visualisation of the masking that is performed in  (global_)forecast-mode (and for others such as temporal interpolation)\n",
    "- [ ] add visualization for BERT\n",
    "\n",
    "### Other supported inference modes:\n",
    "#### BERT Masked Token Mode\n",
    "Neighborhoods are selected randomly based on dates and geographic locations, in which tokens are randomly masked according to a percentage specified in the configuration file.\n",
    "```python\n",
    "mode, options = 'BERT', {'years_val': [2021], 'num_samples_validate': 128, 'with_pytest': True}\n",
    "```\n",
    "Options to configure:\n",
    "- `'years_val'`: Specifies the year from which samples will be drawn.\n",
    "- `'num_samples_validate'`: Number of validation batches, which is also the number of selected neighborhoods. A neighborhood is a patch in space and time which gets processed together (??? improve the sentence)\n",
    "\n",
    "#### BERT Forecast Mode\n",
    "Random neighborhoods are sampled from the global dataset. For each neighborhood, forecasts are generated for a time period set by `forecast_num_tokens` $\\times$ 3 hours.\n",
    "```python\n",
    "mode, options = 'forecast', {'forecast_num_tokens': 2, 'num_samples_validate': 128, 'with_pytest': True}\n",
    "```\n",
    "Previously defined options are used here.\n",
    "\n",
    "#### Temporal Interpolation Mode\n",
    "![forecast](../img/temporal_interpol_1ml.jpg \"Data processing during forecast, with x input, y target output\")\n",
    "\n",
    "In this mode, AtmoRep fills in temporal gaps by predicting missing data for selected neighborhoods.\n",
    "\n",
    "```python\n",
    "mode, options = 'temporal_interpolation', {'idx_time_mask': [5, 6, 7], 'num_samples_validate': 128, 'with_pytest': True}\n",
    "```\n",
    "Key option explained:\n",
    "- `'idx_time_mask'`: Specifies the indices of tokens in a batch of 12 that AtmoRep will attempt to predict.\n",
    "\n",
    "## Step 2 - Process and validate the AtmoRep output\n",
    "\n",
    "The AtmoRep output from the inference step is written to zarr-stores. While the zarr Python package provides a straightforward way to read in the data, the multi-dimensional nature of the data is challenging for subsequent processing. For convenience, we make therefore of a small data interface that reads in the data and turns it into xarry DataArray with labelled dimensions. <br>\n",
    "For more information about xarray, please refer to the [docs](https://docs.xarray.dev/en/latest/getting-started-guide/index.html).\n",
    "\n",
    "Again, we start by importing an auxiliary class and a method for data retrieval and plotting, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35df7c0a-467d-4504-abec-d201cd595139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "from jsc_scripts.utils_hackathon.read_atmorep_data import HandleAtmoRepData\n",
    "from jsc_scripts.utils_hackathon.plotting import plot_global_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f6695-84c4-4f97-90b3-0a5829e28506",
   "metadata": {},
   "source": [
    "Next, we initialize the AtmoRep data handler with the W&B-ID of the inference step executed above and with the results directory of AtmoRep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9cee7-770e-4f1d-bc7b-1c07149cc161",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = user_config.__dict__[\"results\"]    \n",
    "model_id = \"ry906z4c\"                          # adapt here\n",
    "\n",
    "ar_data = HandleAtmoRepData(model_id, input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262aefa-2a45-4ec9-8cc7-32814e3d8d02",
   "metadata": {},
   "source": [
    "Then, we can read in the forecasted and the (ground truth) reference data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162db528-265a-4466-86b7-60c7889eab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_fcst = ar_data.read_data(\"temperature\", \"pred\")\n",
    "da_ref = ar_data.read_data(\"temperature\", \"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfcddd1-9afe-47cb-80ac-e353698cc17a",
   "metadata": {},
   "source": [
    "Let's have a look and check what we obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f7164-4b81-4608-8dd7-2dfe5527fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4cebc-878c-42c7-a6af-90887e959139",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc925931-6203-478e-b5f9-002a24d85797",
   "metadata": {},
   "source": [
    "Since multi-dimensional arrays are hard to grasp, we will plot the data in the follwoing to get some more insight into AtmoRep's forecasts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7645927-72ce-47e6-a353-869a02f4e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# parameters to select the data of interest\n",
    "tidx = -1\n",
    "vlevel = 96\n",
    "offset = -273.15                  # for conveting temperature unit from K to Â°C\n",
    "\n",
    "# set some parameters to customize our plots\n",
    "proj = ccrs.Robinson(central_longitude=0.)      # The projection to display the (global) data\n",
    "transform = ccrs.PlateCarree()                  # transformation-object to be used when processing the data in the plot-routne (don't change!)\n",
    "cmap_name = \"RdBu_r\"                            # colormap \n",
    "\n",
    "fcst, ref = da_fcst.isel({\"datetime\": tidx}).sel({\"ml\": vlevel}) + offset, \\\n",
    "            da_ref.isel({\"datetime\": tidx}).sel({\"ml\": vlevel}) + offset\n",
    "\n",
    "plot_global_data(fcst, \"./test_pred_plot.png\",\n",
    "                 levels=np.arange(-41., 6.), cmap_name=cmap_name, projection=proj, transform=transform)\n",
    "\n",
    "plot_global_data(ref, \"./test_ref_plot.png\",\n",
    "                 levels=np.arange(-41., 6.), cmap_name=cmap_name, projection=proj, transform=transform)\n",
    "\n",
    "plot_global_data(fcst - ref, \"./test_diff_plot.png\", \n",
    "                 levels=np.arange(-3., 3., 0.1), cmap_name=cmap_name, projection=proj, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e217a0f-4175-4500-8a64-595940dbd35c",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "- Plot different variables on different levels\n",
    "- Describe what you see. Can you explain your findings?\n",
    "\n",
    "In the following, we will make our analysis more quantitative. For this, we will compute some basic evaluation metrics such as the RMSE and investigate how the results change with lead time. <br>\n",
    "Again, we don't need to code everything from scratch, but make use of a `Score`-engine that allows computation of several metrics including averaging over user-defined data dimensions. <br>\n",
    "We start by initialising the `Score`-engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9ca85-e793-4202-9bc8-81b2b530a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsc_scripts.utils_hackathon.metrics import Scores\n",
    "\n",
    "score_engine = Scores(da_fcst, da_ref, _, [\"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48db1ef-f0d6-483c-bb4c-2a4c853c4e03",
   "metadata": {},
   "source": [
    "You may consult the doc-string to get further information on the engine. Alternatively, consult the source code (`/p/project/training2445/shared/atmorep/jsc_scripts/utils_hackathon/metrics.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edce801-2837-42b7-9362-763f82ee3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getdoc(Scores.__init__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7144e-7174-41c5-b4fc-4b3bac59c2d3",
   "metadata": {},
   "source": [
    "Let's calculate the RMSE and plot it against leadtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281f629-1dee-432a-abaf-c0e5aaddc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = score_engine(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9836796-acc6-4891-9d48-c1e23adf331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse.sortby(da_fcst[\"datetime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a134c6-9db3-4c56-a6ed-ad1d81fd2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsc_scripts.utils_hackathon.plotting import plot_metric_line\n",
    "import xarray as xr\n",
    "\n",
    "# ML: not yet finished\n",
    "\n",
    "plot_metric_line(rmse.isel({\"ml\": 0}), xr.DataArray(0.), xr.DataArray(0.), {\"RMSE\": \"K\"}, \"./rmse_sample.png\", x_coord=\"datetime\", xlabel=\"leadtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20561d6-2585-4da4-b8b1-d6182765637b",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "- Plot different variables on different levels\n",
    "- Go back to the top of the Jupyter Notebook and re-run the forecasting by modifying the maximum leadtime. How does this affect the reults?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07fe5ce-a739-4e20-b7dc-d42bbf1cb472",
   "metadata": {},
   "source": [
    "## To-Dos @Asma:\n",
    "- [ ] Revise text and improve\n",
    "- [ ] Improve instructions in tasks and check what happens when modifying the maximum leadtime via the `forecast_num_tokens`-parameter when evaluating\n",
    "- [ ] Further/better tasks?\n",
    "    - [ ] Uncover how data is being rea\n",
    "- [ ] Create data for more samples -> to be provied in shared-directory\n",
    "- [ ] Append Jupyter Notebook with evaluation on larger amount of samples (Step 3)\n",
    "\n",
    "## To-Dos @Michael\n",
    "- [ ] Revise data handler with separated lead time dimension and init time of forecast\n",
    "- [ ] Ensure that lead time dimension is consecutive\n",
    "- [ ] Fix and harmonize doc-strings and used methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798476c-2d96-4831-b3f9-d9aad42c38e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hclimrep_hackathon",
   "language": "python",
   "name": "hclimrep_hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
