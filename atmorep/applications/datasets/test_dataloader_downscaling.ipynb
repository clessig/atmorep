{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee3231-d5c2-457d-b8a0-9133da9f52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /p/home/jusers/langguth1/jureca/atmorep_github/atmorep/atmorep/applications/datasets/downscaling_data_loader.py\n",
    "####################################################################################################\n",
    "#\n",
    "#  Copyright (C) 2022\n",
    "#\n",
    "####################################################################################################\n",
    "#\n",
    "#  project     : atmorep\n",
    "#\n",
    "#  author      : atmorep collaboration\n",
    "# \n",
    "#  description :\n",
    "#\n",
    "#  license     :\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import zarr\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os, sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38583b52-c97e-4efe-9baf-e0e516360395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.insert(0, \"/p/project/deepacf/atmo-rep/langguth1/atmo-rep/atmorep/\")\n",
    "sys.path.insert(0, \"/p/project/deepacf/atmo-rep/patnala1/atmorep/\")\n",
    "from atmorep.datasets.normalizer import normalize\n",
    "from atmorep.utils.utils import tokenize, get_weights\n",
    "from atmorep.applications.downscaling.utils.era5_imerg_data_aligner import lat_lon_time_range\n",
    "from atmorep.utils.logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e280158-6f72-4afb-9d6e-074822651985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_to_geolats(lats_math):\n",
    "    \"\"\"\n",
    "    Convert 'mathematical' latitudes as used in AtmoRep to geographical latitudes.\n",
    "    Example conversions:\n",
    "        Mathematical latitude: 0° -> Geographical latitude: 90°N\n",
    "        Mathematical latitude: 180° -> Geographical latitude: -90°N/90°S\n",
    "    :param lats_math: 1D-array with mathematical latitudes\n",
    "    :return lats_geo: 1D-array with geographical latitudes\n",
    "    \"\"\"\n",
    "    lats_geo = np.where(lats_math > 90, -lats_math + 90, np.abs(lats_math - 90.))\n",
    "\n",
    "    return lats_geo\n",
    "\n",
    "def geo_to_mathlats(lats_geo):\n",
    "    \"\"\"\n",
    "    Convert geographical latitude to 'mathematical' latitudes as used in AtmoRep.\n",
    "    Example conversions:\n",
    "        Geographical latitude: 90°N ->  Mathematical latitude: 0°N\n",
    "        Geographical latitude: -90°N -> Mathematical latitude: 180°\n",
    "    :param lats_math: 1D-array with geographical latitudes\n",
    "    :return lats_geo: 1D-array with mathematical latitudes\n",
    "    \"\"\"\n",
    "    lats_math = np.where(lats_geo > 0, np.abs(lats_geo - 90), -lats_geo + 90.)\n",
    "    return lats_math\n",
    "\n",
    "def get_latlon_arr(lat_range, lon_range, res):\n",
    "    lats = np.arange(lat_range[0], lat_range[1] + res/10., res)\n",
    "    lons = np.arange(lon_range[0], lon_range[1] + res/10., res)\n",
    "\n",
    "    return lats, lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b3a15-ec2a-4fd5-a94c-c3192f2cbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultifieldDownscalingSampler( torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, input_file_path, target_file_path, input_fields, target_fields, years, batch_size, n_size,\n",
    "                 num_samples, downscaling_ratio, with_shuffle=False, with_source_idxs = True, with_target_idxs=True) :\n",
    "        '''\n",
    "          Iterable torch dataset for ERA5 to IMERG downscaling task loading data at an arbitrary number of vertical levels\n",
    "    \n",
    "          input_file_path: path to input ERA5 zarr storage\n",
    "          target_file_path: path to input IMERG zarr storage\n",
    "          input_fields: configuration list for input data fields\n",
    "          target_fields: configuration list for input data fields\n",
    "          years: year or list of years from which data should be sampled\n",
    "          batch_size: number of samples per batch\n",
    "          nsize : neighborhood in (tsteps, nlat_ERA5, nlon_ERA5) -> different to MultifieldDataSampler!!!\n",
    "          num_samples: total number of samples to draw\n",
    "          downscaling_ratio: downscaling factor between ERA5 input and IMERG target data\n",
    "          with_shuffle: flag for shuffling samples\n",
    "          with_source_idxs:\n",
    "          with_target_idxs:   \n",
    "        '''       \n",
    "        super(MultifieldDownscalingSampler, self).__init__()\n",
    "\n",
    "        self.input_fields = input_fields\n",
    "        self.target_fields = target_fields\n",
    "        self.n_size = n_size\n",
    "        self.num_samples = num_samples\n",
    "        self.with_source_idxs = with_source_idxs\n",
    "        self.with_target_idxs = with_target_idxs\n",
    "        self.batch_size = batch_size\n",
    "        self.with_shuffle = with_shuffle\n",
    "        self.downscaling_ratio = downscaling_ratio\n",
    "\n",
    "        if not os.path.exists(input_file_path):\n",
    "            FileNotFoundError(f\"Input zarr store {input_file_path} does not exist.\")\n",
    "\n",
    "        if not os.path.exists(input_file_path):\n",
    "            FileNotFoundError(f\"Target zarr store {target_file_path} does not exist.\")\n",
    "\n",
    "        # open zarr stores\n",
    "        self.era5_ds = zarr.group(input_file_path)\n",
    "        self.imerg_ds = zarr.group(target_file_path)\n",
    "\n",
    "        # get coordinate information from ERA5 and IMERG data\n",
    "        self.era5_res = self.era5_ds.attrs['res']\n",
    "        self.imerg_res = self.imerg_ds.attrs['res']\n",
    "\n",
    "        self.era5_times = self.era5_ds[\"time\"]\n",
    "        self.era5_lats = np.array(self.era5_ds['lats'])\n",
    "        self.era5_lons = np.array(self.era5_ds['lons'])\n",
    "\n",
    "        self.imerg_times = self.imerg_ds[\"time\"]\n",
    "        self.imerg_lats = np.array(self.imerg_ds['lats'])\n",
    "        self.imerg_lons = np.array(self.imerg_ds['lons'])\n",
    "        self.imerg_lons = np.where(self.imerg_lons < 0, self.imerg_lons + 360, self.imerg_lons)\n",
    "\n",
    "        self.era5_num_lats = self.era5_ds['lats'].shape[0]\n",
    "        self.era5_num_lons = self.era5_ds['lons'].shape[0]\n",
    "\n",
    "        # spatial shift of first IMERG data point w.r.t. first ERA5 data point\n",
    "        # Note: IMERG grid must already be centered around ERA5 grid accordingly during preprocessing\n",
    "        self.dx_shift = (self.downscaling_ratio - 1)/2.*np.array(self.imerg_res)\n",
    "\n",
    "        # Note: Neighborhood sampling is based on ERA5 indices which serve as anchor points\n",
    "        # Parameters for centered spatial slicing\n",
    "        # In case that the number of grid points is even, the center (anchor) point for the local neighborhood is shifted west/south\n",
    "        # Example nor n_size=4: \n",
    "        #  |           |   |           |\n",
    "        #  x   o   x   x   x   o   x   x \n",
    "        #  |           |   |           |\n",
    "        # The leftmost and rightmost neighborhoods incl. their boundaries | and anchor points o are shown. \n",
    "        nsize_lat_half = int((self.n_size[1] - 1 )/2)\n",
    "        self.nsize_lat = np.asarray([nsize_lat_half, nsize_lat_half])\n",
    "        if self.n_size[1] % 2 == 0:\n",
    "            self.nsize_lat[1] += 1\n",
    "\n",
    "        nsize_lon_half = int((self.n_size[2] - 1 )/2)\n",
    "        self.nsize_lon = np.asarray([nsize_lon_half, nsize_lon_half])\n",
    "        if self.n_size[2] % 2 == 0:\n",
    "            self.nsize_lon[1] += 1\n",
    "\n",
    "        # ensure that sampling is restricted to spatio-temporal index ranges where ERA5 and IMERG data are available\n",
    "        _, tidx_era5, _ = np.intersect1d(self.era5_times, self.imerg_times, return_indices=True)\n",
    "        _, latidx_era5, _ = np.intersect1d(self.era5_lats, self.imerg_lats, return_indices=True)\n",
    "        _, lonidx_era5, _ = np.intersect1d(self.era5_lons, self.imerg_lons, return_indices=True)\n",
    "        \n",
    "        # handle situation where the contigious domain cross the zero meridian\n",
    "        split_idx = np.where(np.diff(lonidx_era5) != 1)[0] + 1    # find indices where the difference between consecutive elements is not 1\n",
    "        \n",
    "        # Add the start and end indices to form ranges\n",
    "        idx_segs = np.split(lonidx_era5, split_idx)\n",
    "        contiguous_segs = [(idx_seg[0], idx_seg[-1]) for idx_seg in idx_segs]\n",
    "\n",
    "        # save ERA5 anchor point indices for sampling\n",
    "        self.range_ilat = np.array([min(latidx_era5) + self.nsize_lat[0], max(latidx_era5) - self.nsize_lat[1]])\n",
    "        self.anchor_ilat = np.arange(*self.range_ilat)\n",
    "        \n",
    "        print(\"***Valid Latitude range***\")\n",
    "        print(min(latidx_era5), max(latidx_era5))\n",
    "        \n",
    "        if len(contiguous_segs) == 2:\n",
    "            range_ilon = [min(contiguous_segs[1]), max(contiguous_segs[0])]\n",
    "        elif len(contiguous_segs) == 1:\n",
    "            range_ilon = [min(lonidx_era5), max(lonidx_era5)]\n",
    "        else:\n",
    "            raise ValueError(f\"More than two splitting indices identified along longitude dimension. Ensure that IMERG-domain is contigious.\")\n",
    "\n",
    "\n",
    "        self.range_ilon = np.array([range_ilon[0] + self.nsize_lon[0], range_ilon[1] - self.nsize_lon[1]])\n",
    "        if self.range_ilon[0] > self.range_ilon[1]:\n",
    "            self.anchor_ilon = np.arange(self.range_ilon[0], self.range_ilon[1] + self.era5_num_lons)\n",
    "            self.anchor_ilon = self.anchor_ilon%self.era5_num_lons\n",
    "        else:\n",
    "            self.anchor_ilon = np.arange(*self.range_ilon)       \n",
    "\n",
    "        print(\"***Valid longitude range***\")\n",
    "        print(range_ilon[0], range_ilon[1])\n",
    "\n",
    "        # Note: sample time-index is not centered, but placed at the end of the time sequence\n",
    "        # add n_size[0] to avoid sampling from time steps at whch no IMERG data is available\n",
    "        self.range_itime = np.array([min(tidx_era5) + self.n_size[0], max(tidx_era5)])\n",
    "        \n",
    "        self.year_base = self.era5_ds['time'][self.range_itime[0]].astype(datetime).year\n",
    "\n",
    "        # get normalizers for input and target data\n",
    "        self.input_normalizers = []\n",
    "\n",
    "        for ifield, field_info in enumerate(input_fields):\n",
    "            corr_type = 'global' if len(field_info) <=6 else field_info[6]\n",
    "            nf_name = 'global_norm' if corr_type == 'global' else 'norm'\n",
    "            self.input_normalizers.append([])\n",
    "            for vl in field_info[2]: \n",
    "                if vl == 0:\n",
    "                    field_idx = self.era5_ds.attrs['fields_sfc'].index( field_info[0])\n",
    "                    n_name = f'normalization/{nf_name}_sfc'\n",
    "                    self.input_normalizers[ifield] += [self.era5_ds[n_name].oindex[ :, :, field_idx]] \n",
    "                else:\n",
    "                    vl_idx = self.era5_ds.attrs['levels'].index(vl)\n",
    "                    field_idx = self.era5_ds.attrs['fields'].index( field_info[0])\n",
    "                    n_name = f'normalization/{nf_name}'\n",
    "                    self.input_normalizers[ifield] += [self.era5_ds[n_name].oindex[ :, :, field_idx, vl_idx] ]\n",
    "\n",
    "\n",
    "        self.target_normalizers = []\n",
    "\n",
    "        for ifield, field_info in enumerate(target_fields):\n",
    "            corr_type = 'global' if len(field_info) <=6 else field_info[6]\n",
    "            nf_name = 'global_norm' if corr_type == 'global' else 'norm'\n",
    "            self.target_normalizers.append([])\n",
    "            for vl in field_info[2]: \n",
    "                if vl == 0:\n",
    "                    field_idx = self.imerg_ds.attrs['fields_sfc'].index( field_info[0])\n",
    "                    n_name = f'normalization/{nf_name}_sfc'\n",
    "                    self.target_normalizers[ifield] += [self.imerg_ds[n_name].oindex[ :, :, field_idx]] \n",
    "                else:\n",
    "                    vl_idx = self.target_ds.attrs['levels'].index(vl)\n",
    "                    field_idx = self.imerg_ds.attrs['fields'].index( field_info[0])\n",
    "                    n_name = f'normalization/{nf_name}'\n",
    "                    self.target_normalizers[ifield] += [self.imerg_ds[n_name].oindex[ :, :, field_idx, vl_idx] ]\n",
    "\n",
    "        # filter data that is not part of the desired period\n",
    "        years_input_file = np.asarray([str(year) for year in self.era5_times[self.range_itime[0]:self.range_itime[1]].astype('datetime64[Y]')])\n",
    "        valid_years_str = [str(year) for year in range(years[0], (years[1] if len(years)>1 else years[0]) + 1)]\n",
    "        mask_year = np.isin(years_input_file, valid_years_str)\n",
    "          \n",
    "        #self.valid_time_indices = np.where(logical_array)[0]\n",
    "        self.anchor_itime = np.arange(*self.range_itime)[mask_year]\n",
    "        self.num_samples = min(self.num_samples, self.anchor_itime.shape[0])\n",
    "            \n",
    "\n",
    "    def shuffle(self):\n",
    "\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        rng_seed = None\n",
    "\n",
    "        if worker_info is not None:\n",
    "            rng_seed = int(time.time()) // (worker_info.id+1) + worker_info.id\n",
    "\n",
    "        rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "        # get random time index\n",
    "        self.idx_perm_t = rng.choice(self.anchor_itime, self.num_samples // self.batch_size, replace=False)\n",
    "\n",
    "        idx_lat, idx_lon = rng.choice(self.anchor_ilat, self.num_samples, replace=False), \\\n",
    "                           rng.choice(self.anchor_ilon, self.num_samples, replace=False)\n",
    "\n",
    "        self.idx_perm_s = np.stack([idx_lat, idx_lon], axis=1)        \n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        if self.with_shuffle:\n",
    "            self.shuffle()\n",
    "\n",
    "        lats, lons = self.era5_lats, self.era5_lons\n",
    "        n_size = self.n_size\n",
    "        ts = 1                             # To-Do: add as parsing parameter!!!\n",
    "\n",
    "        iter_start , iter_end = self.worker_workset()\n",
    "\n",
    "        for bidx in range( iter_start, iter_end):\n",
    "\n",
    "            sources, token_infos = [[] for _ in self.input_fields], [[] for _ in self.input_fields]\n",
    "            targets, target_token_infos = [[] for _ in self.target_fields], [[] for _ in self.target_fields],\n",
    "            sources_infos, source_idxs = [], []\n",
    "            target_infos, target_idxs = [], []\n",
    "\n",
    "            # get matching time index for IMERG dataset\n",
    "            i_bidx_era5 = self.idx_perm_t[bidx]\n",
    "            i_bidx_imerg = np.where(self.era5_times[i_bidx_era5] == self.imerg_times)[0][0]\n",
    "\n",
    "            # get list of time indices for slicing\n",
    "            idxs_t_era5 = list(np.arange( i_bidx_era5 - n_size[0]*ts, i_bidx_era5, ts, dtype=np.int64))\n",
    "            idxs_t_imerg = list(np.arange( i_bidx_imerg - n_size[0]*ts, i_bidx_imerg, ts, dtype=np.int64))\n",
    "\n",
    "            data_t = self.era5_ds['time'][idxs_t_era5[0]:idxs_t_era5[1]].astype(datetime)\n",
    "            \n",
    "            # extract data for time steps at hand\n",
    "            data_era5_tt_sfc = self.era5_ds['data_sfc'][idxs_t_era5]\n",
    "            data_era5_tt = self.era5_ds['data'][idxs_t_era5]\n",
    "\n",
    "            data_imerg_tt_sfc = self.imerg_ds['data_sfc'][idxs_t_imerg]\n",
    "\n",
    "            for sidx in range(self.batch_size):\n",
    "\n",
    "                idx_era5 = self.idx_perm_s[bidx*self.batch_size+sidx]\n",
    "\n",
    "                print(\"Sampled spatial indices:\")\n",
    "                print(idx_era5)\n",
    "                \n",
    "                ilat_range_era5 = np.arange(idx_era5[0] - self.nsize_lat[0], idx_era5[0] + self.nsize_lat[1] + 1)\n",
    "                # handle periodicity in zonal (longitude) direction\n",
    "                ilon_range_era5 = np.arange(idx_era5[1] - self.nsize_lon[0], idx_era5[1] + self.nsize_lon[1] + 1)\n",
    "                \n",
    "                ilon_range_era5 = np.where(ilon_range_era5 < 0, ilon_range_era5 + self.era5_num_lons, ilon_range_era5)\n",
    "                ilon_range_era5 = ilon_range_era5 % self.era5_num_lons\n",
    "\n",
    "                # get corresponding spatial sampling indices for IMERG dataset\n",
    "                ilat_imerg0 = np.where(np.isclose(self.era5_lats[ilat_range_era5[0]] - self.dx_shift[0], self.imerg_lats))[0]\n",
    "                ilon_imerg0 = np.where(np.isclose(self.era5_lons[ilon_range_era5[0]] - self.dx_shift[1], self.imerg_lons))[0]\n",
    "\n",
    "                assert len(ilat_imerg0) == 1, f\"Could not find required IMERG latitude grid point for first ERA5 grid point at {self.era5_lats[ilat_range_era5[0]]} deg\"\n",
    "                assert len(ilon_imerg0) == 1, f\"Could not find required IMERG longitude grid point for first ERA5 grid point at {self.era5_lons[ilon_range_era5[0]]} deg\"\n",
    "\n",
    "                ilat_imerg0, ilon_imerg0 = ilat_imerg0[0], ilon_imerg0[0]\n",
    "\n",
    "                ilat_range_imerg = np.arange(ilat_imerg0, ilat_imerg0 + self.n_size[1]*self.downscaling_ratio)\n",
    "                ilon_range_imerg = np.arange(ilon_imerg0, ilon_imerg0 + self.n_size[2]*self.downscaling_ratio)\n",
    "\n",
    "                # start data retrieval        \n",
    "                sources_infos += [ [ self.era5_ds['time'][ idxs_t_era5 ].astype(datetime), \n",
    "                                     self.era5_lats[ilat_range_era5], self.era5_lons[ilon_range_era5], self.era5_res ] ]\n",
    "                target_infos += [ [ self.imerg_ds['time'][ idxs_t_imerg ].astype(datetime), \n",
    "                                     self.imerg_lats[ilat_range_imerg], self.imerg_lons[ilon_range_imerg], self.imerg_res ] ]\n",
    "        \n",
    "                if self.with_source_idxs :\n",
    "                   source_idxs += [ (idxs_t_era5, ilat_range_era5, ilon_range_era5) ]\n",
    "\n",
    "                if self.with_target_idxs:\n",
    "                   target_idxs += [ (idxs_t_imerg, ilat_range_imerg, ilon_range_imerg) ]\n",
    "        \n",
    "                # extract input ERA5 data\n",
    "                for ifield, field_info in enumerate(self.input_fields):  \n",
    "                  source_lvl, tok_info_lvl  = [], []\n",
    "                  tok_size  = field_info[4]\n",
    "                  num_tokens = field_info[3]\n",
    "                  corr_type = 'global' if len(field_info) <= 6 else field_info[6]\n",
    "                \n",
    "                  for ilevel, vl in enumerate(field_info[2]):\n",
    "                    if vl == 0 : #surface level\n",
    "                      field_idx = self.era5_ds.attrs['fields_sfc'].index( field_info[0])\n",
    "                      data_era5_t = data_era5_tt_sfc[ :, field_idx ]\n",
    "                    else :\n",
    "                      field_idx = self.era5_ds.attrs['fields'].index( field_info[0])\n",
    "                      vl_idx = self.era5_ds.attrs['levels'].index(vl)\n",
    "                      data_era5_t = data_era5_tt[ :, field_idx, vl_idx ]\n",
    "                  \n",
    "                    source_data, tok_info = [], []\n",
    "                    # extract data, normalize and tokenize\n",
    "                    cdata = data_era5_t[ ... , ilat_range_era5[:, np.newaxis], ilon_range_era5[np.newaxis, :]]\n",
    "\n",
    "                    # NOTE: uncomment/activate the following after testing\n",
    "                    normalizer = self.input_normalizers[ifield][ilevel]\n",
    "                    #if False: #corr_type != 'global': \n",
    "                    if corr_type != 'global': \n",
    "                      if ilat_range_era5[0] < ilat_range_era5[-1] and ilon_range_era5[0] < ilon_range_era5[-1]:\n",
    "                        lat_max, lat_min = max(ilat_range_era5), min(ilat_range_era5)\n",
    "                        lon_max, lon_min = max(ilon_range_era5), min(ilon_range_era5)\n",
    "                        normalizer = normalizer[:,:,lat_min:lat_max+1,lon_min:lon_max+1]\n",
    "                      else:\n",
    "                        normalizer = normalizer[ ... , ilat_range_era5[:,np.newaxis], ilon_range_era5[np.newaxis,:]]\n",
    " \n",
    "                    cdata = normalize(cdata, normalizer, sources_infos[-1][0], year_base = self.year_base)\n",
    "                    \n",
    "                    source_data = tokenize( torch.from_numpy( cdata), tok_size )  \n",
    "                    \n",
    "                    # NOTE: Remove this after testing\n",
    "                    #source_data = cdata\n",
    "                    # token_infos uses center of the token: *last* datetime and center in space\n",
    "                    dates = self.era5_ds['time'][ idxs_t_era5 ].astype(datetime)\n",
    "                    cdates = dates[tok_size[0]-1::tok_size[0]]\n",
    "                    # use -1 is to start days from 0\n",
    "                    dates = [(d.year, d.timetuple().tm_yday-1, d.hour) for d in cdates] \n",
    "                    lats_sidx = self.era5_lats[ilat_range_era5][ tok_size[1]//2 :: tok_size[1] ]\n",
    "                    lons_sidx = self.era5_lons[ilon_range_era5][ tok_size[2]//2 :: tok_size[2] ]\n",
    "                    # tensor product for token_infos\n",
    "                    tok_info += [[[[[ year, day, hour, vl, lat, lon, vl, self.era5_res[0]] for lon in lons_sidx]\n",
    "                                                                                      for lat in lats_sidx]\n",
    "                                                                          for (year, day, hour) in dates]]\n",
    "        \n",
    "                    source_lvl += [source_data]\n",
    "                    tok_info_lvl += [ torch.tensor(tok_info, dtype=torch.float32).flatten( 1, -2)]\n",
    "                      \n",
    "                  sources[ifield] += [ torch.stack(source_lvl, 0) ]\n",
    "                  token_infos[ifield] += [ torch.stack(tok_info_lvl, 0) ]\n",
    "\n",
    "                for ifield, field_info in enumerate(self.target_fields):  \n",
    "                  target_lvl, tok_info_lvl  = [], []\n",
    "                  target_tok_size  = field_info[4]\n",
    "                  target_num_tokens = field_info[3]\n",
    "                  corr_type = 'global' if len(field_info) <= 6 else field_info[6]  \n",
    "                \n",
    "                  for ilevel, vl in enumerate(field_info[2]):\n",
    "                    if vl == 0 : #surface level\n",
    "                        field_idx = self.imerg_ds.attrs['fields_sfc'].index( field_info[0])\n",
    "                        data_t_imerg = data_imerg_tt_sfc[:,field_idx]\n",
    "                    else :\n",
    "                        raise ValueError(f\"No multi-level data available in IMERG-dataset. Please check the target_field-configuration.\")\n",
    "                  \n",
    "                    target_data, tok_info = [], []\n",
    "                    # extract data, normalize and tokenize\n",
    "                    cdata = data_t_imerg[ ... , ilat_range_imerg[:,np.newaxis], ilon_range_imerg[np.newaxis,:]]\n",
    "\n",
    "                    # NOTE: uncomment the following after testing\n",
    "                    normalizer = self.target_normalizers[ifield][ilevel]\n",
    "                    if corr_type != 'global': \n",
    "                      if ilat_range_imerg[0] < ilat_range_imerg[-1] and ilon_range_imerg[0] < ilon_range_imerg[-1]:\n",
    "                        lat_max, lat_min = max(ilat_range_imerg), min(ilat_range_imerg)\n",
    "                        lon_max, lon_min = max(ilon_range_imerg), min(ilon_range_imerg)\n",
    "                        normalizer = normalizer[:,:,lat_min:lat_max+1,lon_min:lon_max+1]\n",
    "                      else:\n",
    "                        normalizer = normalizer[ ... , ilat_range_imerg[:,np.newaxis], ilon_range_imerg[np.newaxis,:]]\n",
    "                    cdata = normalize(cdata, normalizer, sources_infos[-1][0], year_base = self.year_base)\n",
    "                    \n",
    "                    target_data = tokenize(torch.from_numpy(cdata), target_tok_size ) \n",
    "\n",
    "                    # NOTE: Remove this after testing\n",
    "                    #target_data = cdata \n",
    "                      \n",
    "                    # token_infos uses center of the token: *last* datetime and center in space\n",
    "                    dates = self.imerg_ds['time'][ idxs_t_imerg ].astype(datetime)\n",
    "                    cdates = dates[tok_size[0]-1::tok_size[0]]\n",
    "                    # use -1 is to start days from 0\n",
    "                    dates = [(d.year, d.timetuple().tm_yday-1, d.hour) for d in cdates] \n",
    "                    lats_sidx = self.imerg_lats[ilat_range_imerg][ target_tok_size[1]//2 :: target_tok_size[1] ]\n",
    "                    lons_sidx = self.imerg_lons[ilon_range_imerg][ target_tok_size[2]//2 :: target_tok_size[2] ]\n",
    "                    # tensor product for token_infos\n",
    "                    tok_info += [[[[[ year, day, hour, vl, lat, lon, vl, self.imerg_res[0]] for lon in lons_sidx]\n",
    "                                                                                            for lat in lats_sidx]\n",
    "                                                                          for (year, day, hour) in dates]]\n",
    "        \n",
    "                    target_lvl += [target_data]\n",
    "                    tok_info_lvl += [ torch.tensor(tok_info, dtype=torch.float32).flatten( 1, -2)]      \n",
    "                  targets[ifield] += [ torch.stack(target_lvl, 0) ]\n",
    "                  target_token_infos[ifield] += [ torch.stack(tok_info_lvl, 0) ]\n",
    "\n",
    "            sources = [torch.stack(sources_field) for sources_field in sources]\n",
    "            token_infos = [torch.stack(tis_field) for tis_field in token_infos]\n",
    "\n",
    "            targets = [torch.stack(targets_field) for targets_field in targets]\n",
    "            \n",
    "            yield ((sources, token_infos), (source_idxs, sources_infos), targets, (target_idxs,target_infos))          # (targets, target_token_infos))\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.num_samples // self.batch_size\n",
    "\n",
    "\n",
    "    def worker_workset( self) :\n",
    "\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "\n",
    "        if worker_info is None: \n",
    "            iter_start = 0\n",
    "            iter_end = self.num_samples\n",
    "    \n",
    "        else:  \n",
    "            # split workload\n",
    "            per_worker = len(self) // worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = int(worker_id * per_worker)\n",
    "            iter_end = int(iter_start + per_worker)\n",
    "            if worker_info.id+1 == worker_info.num_workers :\n",
    "                iter_end = len(self)\n",
    "\n",
    "        return iter_start, iter_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d446cc-7e0b-47d1-ade2-38ad1b935b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atmorep.utils.utils import Config\n",
    "\n",
    "cf = Config()\n",
    "\n",
    "#cf.model_id = \"3kdutwqb\" \n",
    "cf.fields = [[\"velocity_u\", [1, 2048, [\"velocity_v\", \"temperature\", \"specific_humidity\", \"velocity_z\"], 0, [\"3k6e6p7o\", 141]], [96, 105, 114, 123, 137], [12, 6, 12], [3, 9, 9], [0.7, 0.9, 0.2, 0.05], \"local\"], \n",
    "             [\"velocity_v\", [1, 2048, [\"velocity_u\", \"temperature\", \"specific_humidity\", \"velocity_z\"], 1, [\"brxmevmt\", 141]], [96, 105, 114, 123, 137], [12, 6, 12], [3, 9, 9], [0.7, 0.9, 0.2, 0.05], \"local\"], \n",
    "             [\"specific_humidity\", [1, 2048, [\"velocity_u\", \"velocity_v\", \"velocity_z\", \"temperature\", \"total_precip\"], 2, [\"ctxc97nr\", 128]], [96, 105, 114, 123, 137], [12, 6, 12], [3, 9, 9], [0.85, 0.9, 0.2, 0.05], \"local\"], \n",
    "             [\"velocity_z\", [1, 1024, [\"velocity_u\", \"velocity_v\", \"temperature\"], 0, [\"15oisw8d\", 273]], [96, 105, 114, 123, 137], [12, 6, 12], [3, 9, 9], [0.65, 0.9, 0.2, 0.05], \"global\"],\n",
    "             [\"temperature\", [1, 1536, [\"velocity_u\", \"velocity_v\", \"velocity_z\", \"specific_humidity\"], 3, [\"3qou60es\", 327]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.85, 0.9, 0.2, 0.05], \"local\"],\n",
    "             [\"total_precip\", [1, 1536, [\"velocity_u\", \"velocity_v\", \"velocity_z\", \"specific_humidity\"], 3, [\"3kdutwqb\", 900]], [0], [12, 6, 12], [3, 9, 9], [0.5, 0.9, 0.1, 0.05]]]\n",
    "cf.input_fields = cf.fields\n",
    "\n",
    "cf.downscaling_ratio = 3\n",
    "cf.fields_downscaling = [ ['total_precip', \n",
    "                        [1,1536,[\"velocity_u\",\"velocity_v\",\"specific_humidity\"]],\n",
    "                        [0],\n",
    "                        [12,6,12],\n",
    "                        [3,9*cf.downscaling_ratio,9*cf.downscaling_ratio], \n",
    "                        1.0 ] ]\n",
    "cf.target_fields = cf.fields_downscaling\n",
    "cf.input_file_path = \"/p/scratch/atmo-rep/data/era5_1deg/months/era5_y1979_2021_res025_chunk8.zarr\"\n",
    "cf.target_file_path = \"/p/scratch/atmo-rep/data/imerg/imerg_regridded/imerg_regrid_y2003_2021_res083_chunk8.zarr\"\n",
    "cf.years_train = [2003,2020]\n",
    "cf.years_val = [2021]  #[2018] \n",
    "cf.month = None\n",
    "cf.batch_size = 8\n",
    "cf.num_samples_per_epoch = 24\n",
    "cf.n_size = [36, 9*6, 9*12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5339b5-1a2b-4475-8342-706efab52bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.downscaling_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f9ebc-d34a-4a5a-a675-a0d15100bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaling_dataset = MultifieldDownscalingSampler(\n",
    "                    cf.input_file_path,\n",
    "                    cf.target_file_path,\n",
    "                    cf.fields,\n",
    "                    cf.target_fields,\n",
    "                    cf.years_val,\n",
    "                    cf.batch_size,\n",
    "                    cf.n_size,\n",
    "                    cf.num_samples_per_epoch,\n",
    "                    cf.downscaling_ratio,\n",
    "                    with_shuffle=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04310aa2-89cc-46fb-b8b5-bdd0426b522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(downscaling_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f435dc3-7f98-406d-9824-ed5a8bd5a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_ds = iter(downscaling_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c4357-9a9a-4cf5-8281-c9cca749ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sources, token_infos), (source_idxs, sources_infos), targets, (target_idxs,target_infos) = next(iter_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693f658-9668-4cad-9cb5-50f254bd127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "era5_sample = sources[-1][sample_idx,0]\n",
    "imerg_sample = targets[0][sample_idx,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f9452-9a33-4049-8e98-cfacfcb5fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize( data):\n",
    "    data = data.permute( [*np.arange( len(data.shape)-5), -3, -5, -2, -4, -1])\n",
    "    data = data.reshape( [*data.shape[:-6], np.prod( data.shape[-6:-4]), # time\n",
    "                                          np.prod( data.shape[-4:-2]), # lat\n",
    "                                          np.prod( data.shape[-2:])])  #lon\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49992e-a2a3-4440-91ba-2be491d9c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize=True\n",
    "normalize=True\n",
    "\n",
    "if tokenize:\n",
    "    era5_sample = detokenize(era5_sample)\n",
    "    imerg_sample = detokenize(imerg_sample)\n",
    "\n",
    "if normalize:\n",
    "    from atmorep.datasets.normalizer import denormalize\n",
    "    era5_sample = denormalize(era5_sample, downscaling_dataset.input_normalizers[5][0], sources_infos[sample_idx][0],downscaling_dataset.year_base)\n",
    "    imerg_sample = denormalize(imerg_sample, downscaling_dataset.target_normalizers[0][0], target_infos[sample_idx][0],downscaling_dataset.year_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292413b7-f7f6-4029-b1de-5b3d5ef19d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test for temp parameter as normalizer set there is local whereas for precipitation it is global\n",
    "\n",
    "# era5_temp_sample = sources[-1][sample_idx,0]\n",
    "\n",
    "# if tokenize:\n",
    "#     era5_temp_sample = detokenize(era5_temp_sample)\n",
    "\n",
    "# if normalize:\n",
    "#     era5_temp_sample = denormalize(era5_temp_sample, downscaling_dataset.input_normalizers[4][0][:,:,source_idxs[sample_idx][1][:,np.newaxis],source_idxs[sample_idx][2][np.newaxis,:]], sources_infos[sample_idx][0], downscaling_dataset.year_base)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a18b7-e4f2-4eb7-8319-27325812110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_era5 = xr.DataArray(era5_sample*1000., dims=[\"time\", \"lat\", \"lon\"], \n",
    "                           coords={\"time\": sources_infos[sample_idx][0], \"lat\": math_to_geolats(sources_infos[sample_idx][1]), \"lon\": np.where(sources_infos[sample_idx][2] >180.0,\n",
    "                                                                                                                                               sources_infos[sample_idx][2] - 360.0,\n",
    "                                                                                                                                               sources_infos[sample_idx][2])})\n",
    "\n",
    "precip_imerg = xr.DataArray(imerg_sample, dims=[\"time\", \"lat\", \"lon\"], \n",
    "                           coords={\"time\": target_infos[sample_idx][0], \"lat\": math_to_geolats(target_infos[sample_idx][1]),\n",
    "                                   \"lon\": np.where(target_infos[sample_idx][2] > 180., target_infos[sample_idx][2] - 360.0, target_infos[sample_idx][2])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7cbb1-115d-41b7-adb6-2ab5c63c7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_era5['lat'], precip_imerg['lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3edc28-21e5-4343-8441-7a21849083f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_era5['lon'], precip_imerg['lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da42d3-9b78-4744-8a22-984cb92a6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# parameters for plotting\n",
    "tidx = 9\n",
    "val_range = {\"vmin\": 0, \"vmax\": 2.}\n",
    "\n",
    "precip_imerg.isel({\"time\": tidx}).plot(**val_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5b4c4-bad6-4049-9a53-cdab078a10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_era5.isel({\"time\": tidx}).plot(**val_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f380c0a-e75f-48b1-a081-03b318569f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "isample = 3\n",
    "\n",
    "imerg_sample = (targets[0].numpy())[isample, 0, ...]\n",
    "era5_sample = (sources[-1].numpy())[isample, 0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1fb23-13bd-4eb1-8855-00c7883b7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year, day, hour = tuple(token_infos[-1].numpy()[isample, 0, 0, 0][0:3])\n",
    "\n",
    "date_start = pd.to_datetime(year * 1000 + day, format='%Y%j')\n",
    "date_start = date_start + pd.to_timedelta(hour, unit='h') - pd.to_timedelta(1, unit='h')\n",
    "all_dates = pd.date_range(date_start, periods=36, freq=\"1h\")\n",
    "\n",
    "# sanity check\n",
    "year, day, hour = tuple(token_infos[-1].numpy()[isample,0,0,-1][0:3])\n",
    "\n",
    "date_end = pd.to_datetime(year * 1000 + day, format='%Y%j')\n",
    "date_end = date_end + pd.to_timedelta(hour, unit='h') + pd.to_timedelta(1, unit='h')\n",
    "\n",
    "assert all_dates[-1] == date_end, \"Inconsistent date information retrieved from token info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994935b-f22d-48c4-8abc-1935c3d9d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start, date_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c27104-6c9e-4fce-90f4-695135d32d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min, lat_max = token_infos[-1].numpy()[0, 0, 0, 0][[4]], token_infos[-1].numpy()[0, 0, 0, -1][[4]]\n",
    "lon_min, lon_max = token_infos[-1].numpy()[0, 0, 0, 0][[5]], token_infos[-1].numpy()[0, 0, 0, -1][[5]]\n",
    "\n",
    "if lon_min > lon_max:\n",
    "    lon_min -= 360.\n",
    "\n",
    "lat_range_era5 = [lat_min[0] - 4*0.25, lat_max[0] + 4*.25]\n",
    "lon_range_era5 = [lon_min[0] - 4*0.25, lon_max[0] + 4*.25]\n",
    "\n",
    "lat_range_imerg = [lat_range_era5[0] - 0.25/3, lat_range_era5[1] + 0.25/3]\n",
    "lon_range_imerg = [lon_range_era5[0] - 0.25/3, lon_range_era5[1] + 0.25/3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c403f-1e73-4667-b4f1-37d1ae5f6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats_era5, lons_era5 = get_latlon_arr(lat_range_era5, lon_range_era5, .25)\n",
    "lats_imerg, lons_imerg = get_latlon_arr(lat_range_imerg, lon_range_imerg, .25/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b10ae7-6d8e-4e63-851c-8a23e207e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_era5 = xr.DataArray(era5_sample*1000., dims=[\"time\", \"lat\", \"lon\"], \n",
    "                           coords={\"time\": all_dates, \"lat\": math_to_geolats(lats_era5), \"lon\": lons_era5})\n",
    "\n",
    "precip_imerg = xr.DataArray(imerg_sample, dims=[\"time\", \"lat\", \"lon\"], \n",
    "                           coords={\"time\": all_dates, \"lat\": math_to_geolats(lats_imerg), \"lon\": lons_imerg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c566f4c-d24a-45f4-b10b-257a5fd457ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# parameters for plotting\n",
    "tidx = 7\n",
    "val_range = {\"vmin\": 0, \"vmax\": 2.}\n",
    "\n",
    "precip_imerg.isel({\"time\": tidx}).plot(**val_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff5f36-e35a-434b-acac-343c6b5d5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_era5.isel({\"time\": tidx}).plot(**val_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628414eb-ecfc-4939-abd9-3844f6c899b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datadir = Path(\"/p/project1/deepacf/atmo-rep/patnala1/nc_files_new\")\n",
    "ds_imerg = xr.open_dataset(datadir.joinpath(\"imerg_25722_44_78.nc\"))\n",
    "ds_era5 = xr.open_dataset(datadir.joinpath(\"era_5_25722_109_2.nc\"))\n",
    "\n",
    "precip_era5 = ds_era5[\"total_precip\"]\n",
    "precip_imerg = ds_imerg[\"total_precip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49077a-9c8f-4fa0-a6c8-96ff6472ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be6237-529d-4bce-8708-5e56dc7f2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidx = 0\n",
    "\n",
    "ds_imerg[\"total_precip\"].isel({\"time\": tidx}).plot(vmin=0., vmax=2.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c666866-2627-460a-a729-50f6acc46289",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5[\"lons\"] = ds_era5[\"lons\"].where(ds_era5[\"lons\"] < 180., ds_era5[\"lons\"] - 360.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0fc22-80b0-475f-841f-105fa3bf9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5[\"total_precip\"].isel({\"time\": tidx}).plot(vmin=0., vmax=2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95676011-8fc5-4452-82a2-04e1f1c55e95",
   "metadata": {},
   "source": [
    "# Plot data from raw datafiles for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d2579-05e7-47db-b52b-0dd42fa5f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_imerg_raw = Path(\"/p/scratch/atmo-rep/data/imerg/imerg_regridded\")\n",
    "datadir_era5_raw = Path(\"/p/scratch/atmo-rep/data/era5/new_structure/total_precip/ml0\")\n",
    "\n",
    "date_now = pd.to_datetime(precip_era5[\"time\"][tidx].values)\n",
    "ym_str = date_now.strftime(\"y%Y_m%m\")\n",
    "\n",
    "fname_imerg_raw = datadir_imerg_raw.joinpath(f\"3B-HHR.MS.MRG.3IMERG.{ym_str}.nc\")\n",
    "fname_era5_raw = datadir_era5_raw.joinpath(f\"era5_total_precip_{ym_str}_ml0.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf05441-5a3a-4b82-99f8-c39adf96d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Read raw IMERG-data from {fname_imerg_raw}\")\n",
    "ds_imerg_raw = xr.open_dataset(fname_imerg_raw)\n",
    "\n",
    "print(f\"Read raw ERA5-data from {fname_era5_raw}\")\n",
    "ds_era5_raw = xr.open_dataset(fname_era5_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea073d36-df68-40a5-a274-83b5fb1dfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "\n",
    "precip_imerg_raw = ds_imerg_raw[\"precipitation\"].sel({\"lat\": precip_imerg['lat'], \"lon\": precip_imerg['lon'], \n",
    "                                                     \"time\": date_now + pd.Timedelta(offset,\"h\")}, method='nearest')\n",
    "\n",
    "precip_era5_raw = ds_era5_raw[\"tp\"].sel({\"latitude\": precip_era5['lat'], \"longitude\": precip_era5['lon'], \n",
    "                                         \"time\": date_now + pd.Timedelta(offset,\"h\")})*1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d6570-5cf2-4f5e-8438-ebcad7bea977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "precip_imerg_raw.plot(vmin=0., vmax=2.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc9d4c-7478-43f2-bbc2-ec48ae8b1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_era5_raw.plot(vmin=0., vmax=2.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386764b-6c1d-42e4-bf68-9edac663ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.amax(np.abs(precip_era5_raw - precip_era5.isel({\"time\": tidx}))) < 5.e-04, \"ERA5-data from file and from torch dataset are different.\"\n",
    "assert np.amax(np.abs(precip_imerg_raw - precip_imerg.isel({\"time\": tidx}))) < 1.e-06, \"IMERG-data from file and from torch dataset are different.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e487ef6-d6ad-4caf-bc97-0b1889747a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atmorep_cluster",
   "language": "python",
   "name": "atmorep_cluster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
